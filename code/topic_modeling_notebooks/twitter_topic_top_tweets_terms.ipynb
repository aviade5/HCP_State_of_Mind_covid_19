{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = r'D:\\iliapl\\topic_modeling\\data\\databases\\bad_actors_Coronavirus_Project_POI_Followers_13-06-20.db'\n",
    "twitter_model_path = r'D:/iliapl/topic_modeling/data/output_data/hpc_generated/with_keywords/POI_Followers_13-06-20_PERSON_ONLY_V10_TOP40PERCENT_WITH_KEYWORDS_25TOPICS/'\n",
    "location_analysis_path = r'D:\\iliapl\\topic_modeling\\data\\output_data\\twitter_location_analysis\\POI_Followers_13-06-20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "model_dict = Dictionary.load('{}/dict.id2word'.format(twitter_model_path))\n",
    "corpus = MmCorpus('{}/corpus.mm'.format(twitter_model_path))\n",
    "model = LdaModel.load('{}/lda.model'.format(twitter_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('{}/post_id_bow_dict.json'.format(twitter_model_path), 'r') as file_handle:\n",
    "    tweet_id_bow_dict = json.load(file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded id -> topic dictionary in 5492.541023015976 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import ast\n",
    "\n",
    "T = time.time()\n",
    "\n",
    "tweet_id_topic_dict = {}\n",
    "\n",
    "if not os.path.isfile('{}/tweet_topic_map.json'.format(twitter_model_path)):\n",
    "    for tweet_id, bow in tweet_id_bow_dict.items():\n",
    "        topics = model.get_document_topics(bow)\n",
    "        tweet_id_topic_dict[tweet_id] = max(topics, key=lambda tup: tup[1])\n",
    "        \n",
    "        # so we can save it as json\n",
    "        tweet_id_topic_dict[tweet_id] = (tweet_id_topic_dict[tweet_id][0], round(tweet_id_topic_dict[tweet_id][1], 3))\n",
    "    with open('{}/tweet_topic_map.json'.format(twitter_model_path), 'w') as file_handle:\n",
    "        json.dump({tweet_id : str(topic) for (tweet_id, topic) in tweet_id_topic_dict.items()}, file_handle)\n",
    "else:\n",
    "    with open('{}/tweet_topic_map.json'.format(twitter_model_path), 'r') as file_handle:\n",
    "        tweet_id_topic_dict = json.load(file_handle)\n",
    "        tweet_id_topic_dict = {tweet_id : ast.literal_eval(tup_str) for (tweet_id, tup_str) in tweet_id_topic_dict.items()}\n",
    "    \n",
    "T = time.time() - T\n",
    "print('Loaded id -> topic dictionary in {} seconds'.format(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded id -> country/state dictionaries in 80.3342056274414 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import ast\n",
    "\n",
    "T = time.time()\n",
    "\n",
    "tweet_id_country_dict = {}\n",
    "\n",
    "with open('{}/tweet_countries.json'.format(location_analysis_path), 'r') as file_handle:\n",
    "    tweet_id_country_dict = json.load(file_handle)\n",
    "\n",
    "tweet_id_state_dict = {}\n",
    "\n",
    "with open('{}/tweet_states.json'.format(location_analysis_path), 'r') as file_handle:\n",
    "    tweet_id_state_dict = json.load(file_handle)\n",
    "\n",
    "with open('{}/state_code_name.json'.format(location_analysis_path), 'r') as file_handle:\n",
    "    state_code_name_dict = json.load(file_handle)\n",
    "    \n",
    "T = time.time() - T\n",
    "print('Loaded id -> country/state dictionaries in {} seconds'.format(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "\n",
    "conn = sql.connect(database_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top N tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_topn_tweets_for_topic(topic_id, topn):\n",
    "    topic_tweets = [(tweet_id, prob) for (tweet_id, (tweet_topic_id, prob)) in tweet_id_topic_dict.items() if tweet_topic_id == topic_id]\n",
    "    sorted_tweets = sorted(topic_tweets, key=lambda tup:tup[1], reverse=True)\n",
    "    \n",
    "    # we retrieve this number of tweets in hope that there will actually be topn distinct ones out of them\n",
    "    max_tweets_to_retrieve = 200\n",
    "    \n",
    "    tweet_contents = []\n",
    "    query = \"SELECT post_id, author, date, url, content FROM posts WHERE post_id IN {}\".format(\n",
    "        str(tuple([tweet_info[0] for tweet_info in sorted_tweets[:max_tweets_to_retrieve]])))\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    results = cur.execute(query).fetchall()\n",
    "\n",
    "    results = [(result[0], tweet_id_topic_dict[result[0]][1], result[1], result[2], result[3], re.sub('RT @.*: ', '', result[4]).replace('\\n', ' ').replace('\\r\\n', ' ').replace('\\t', ' ')) for result in results]\n",
    "    sorted_results = sorted(results, key=lambda tup: tup[1], reverse=True)\n",
    "    unique_contents = set([result[5] for result in sorted_results])\n",
    "        \n",
    "    topn_results = []\n",
    "    for result in sorted_results:\n",
    "        if result[5] in unique_contents:\n",
    "            topn_results.append(result)\n",
    "            unique_contents.remove(result[5])\n",
    "            if len(topn_results) == topn:\n",
    "                break\n",
    "                \n",
    "    if len(topn_results) < topn:\n",
    "        print('Could not get requested topn for topic {}! Only got {}/{}! Increase max_tweets_to_retrieve value!'.format(\n",
    "            topic_id, len(topn_results), topn))\n",
    "    \n",
    "    for result in topn_results:\n",
    "        post_id = result[0]\n",
    "        if post_id not in tweet_id_country_dict:\n",
    "            location = 'unknown'\n",
    "        else:\n",
    "            if post_id in tweet_id_state_dict:\n",
    "                location = '{}, United States'.format(state_code_name_dict[tweet_id_state_dict[post_id]])\n",
    "            else:\n",
    "                location = tweet_id_country_dict[post_id]\n",
    "            \n",
    "        tweet_contents.append((post_id, result[1], result[2], location, result[3], result[4], result[5]))\n",
    "    return tweet_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not get requested topn for topic 5! Only got 66/100! Increase max_tweets_to_retrieve value!\n",
      "Could not get requested topn for topic 15! Only got 35/100! Increase max_tweets_to_retrieve value!\n",
      "Could not get requested topn for topic 21! Only got 75/100! Increase max_tweets_to_retrieve value!\n",
      "Could not get requested topn for topic 23! Only got 89/100! Increase max_tweets_to_retrieve value!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "topn = 100\n",
    "\n",
    "top_tweets = []\n",
    "for topic_id in range(NUM_TOPICS):\n",
    "    top_tweets += [(topic_id, *tweet_info) for tweet_info in sorted(get_topn_tweets_for_topic(topic_id, topn), key=lambda tup: tup[1], reverse=True)]\n",
    "\n",
    "df = pd.DataFrame(top_tweets, columns=['topic_id', 'post_id', 'fit_value', 'author_name', 'location', 'date', 'url', 'content'])\n",
    "df.to_csv('{}/top{}_tweets.csv'.format(twitter_model_path, topn), encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top words\n",
    "\n",
    "topn = 50\n",
    "\n",
    "top_words = []\n",
    "for topic_id in range(NUM_TOPICS):\n",
    "    topic_top_words = model.get_topic_terms(topic_id, topn=topn)\n",
    "    topic_top_words = [(model_dict[word_id], prob) for (word_id, prob) in topic_top_words]\n",
    "    topic_top_words = [(topic_id, *tup) for tup in topic_top_words]\n",
    "    top_words += topic_top_words\n",
    "\n",
    "df = pd.DataFrame(top_words, columns=['topic_id', 'word', 'fit_value'])\n",
    "df.to_csv('{}/top_terms.csv'.format(twitter_model_path), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All tweets for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top terms per day/week/month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded id -> date dictionary in 1066.299147605896 seconds\n"
     ]
    }
   ],
   "source": [
    "import sqlite3 as sql\n",
    "import os\n",
    "\n",
    "if not os.path.exists(twitter_model_path):\n",
    "    os.makedirs(twitter_model_path)\n",
    "\n",
    "T = time.time()\n",
    "\n",
    "if not os.path.isfile('{}/tweet_date_map.json'.format(twitter_model_path)):\n",
    "    conn = sql.connect(database_path)\n",
    "    cur = conn.cursor()\n",
    "    query = 'SELECT post_id, date FROM posts'\n",
    "    tweet_id_dates = cur.execute(query).fetchall()\n",
    "    tweet_id_date_dict = {tweet_id : date for (tweet_id, date) in tweet_id_dates if tweet_id in tweet_id_bow_dict}\n",
    "    with open('{}/tweet_date_map.json'.format(twitter_model_path), 'w') as file_handle:\n",
    "        json.dump(tweet_id_date_dict, file_handle)\n",
    "else:\n",
    "    with open('{}/tweet_date_map.json'.format(twitter_model_path), 'r') as file_handle:\n",
    "        tweet_id_date_dict = json.load(file_handle)\n",
    "\n",
    "\n",
    "T = time.time() - T\n",
    "print('Loaded id -> date dictionary in {} seconds'.format(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "tweet_id_date_dict = {tweet_id : datetime.datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S') for (tweet_id, date_str) in tweet_id_date_dict.items()}\n",
    "start_date = min([date for (_, date) in tweet_id_date_dict.items()]).date()\n",
    "end_date = max([date for (_, date) in tweet_id_date_dict.items()]).date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topn_most_popular(word_counts, topn):\n",
    "    tuple_list = list(word_counts.items())\n",
    "    sorted_list = sorted(tuple_list, key=lambda tup:tup[1], reverse=True)\n",
    "    return sorted_list[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_counts_per_day = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topn_words_per_day(start_date, end_date, topn):\n",
    "\n",
    "    topn_most_popular_words_per_day = []\n",
    "    \n",
    "    current_date = start_date\n",
    "    while start_date <= current_date <= end_date:\n",
    "        date_word_counts = {}\n",
    "        for tweet_id in tweet_id_date_dict:\n",
    "            if tweet_id_date_dict[tweet_id].date() == current_date:\n",
    "                for word_id_count_list in tweet_id_bow_dict[tweet_id]:\n",
    "                    word_id = word_id_count_list[0]\n",
    "                    count = word_id_count_list[1]\n",
    "                    word = model_dict[word_id]\n",
    "                    date_word_counts[word] = date_word_counts.get(word, 0) + count\n",
    "        all_word_counts_per_day.append(date_word_counts)\n",
    "        topn_most_popular_words_per_day.append(get_topn_most_popular(date_word_counts, topn))\n",
    "                    \n",
    "        current_date += datetime.timedelta(days=1)\n",
    "    return topn_most_popular_words_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flattened_list(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 50\n",
    "most_popular_words_per_day = get_topn_words_per_day(start_date, end_date, topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "flattened_list = get_flattened_list(most_popular_words_per_day)\n",
    "index = []\n",
    "for i in range(len(flattened_list)):\n",
    "    if i % topn == 0:\n",
    "        index.append(start_date + datetime.timedelta(days=int(i / topn)))\n",
    "    else:\n",
    "        index.append('')\n",
    "pd.DataFrame(flattened_list, columns=['word', 'count'],\n",
    "             index=index).to_csv('{}/top{}_words_daily.csv'.format(twitter_model_path, topn), encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def sum_dictionary_values(dict_list):\n",
    "    return dict(sum((Counter(dict(x)) for x in dict_list), Counter()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekly\n",
    "\n",
    "divided_list = [all_word_counts_per_day[x:x+7] for x in range(0, (end_date - start_date).days + 1, 7)]\n",
    "weekly_counts = []\n",
    "for week in divided_list:\n",
    "    weekly_counts.append(sum_dictionary_values(week))\n",
    "\n",
    "\n",
    "most_popular_words_per_week = [get_topn_most_popular(weekly, topn) for weekly in weekly_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list = get_flattened_list(most_popular_words_per_week)\n",
    "index = []\n",
    "for i in range(len(flattened_list)):\n",
    "    if i % topn == 0:\n",
    "        index.append(start_date + datetime.timedelta(days=7 * int(i / topn)))\n",
    "    else:\n",
    "        index.append('')\n",
    "pd.DataFrame(flattened_list, columns=['word', 'count'],\n",
    "             index=index).to_csv('{}/top{}_words_weekly.csv'.format(twitter_model_path, topn), encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly\n",
    "\n",
    "monthly_counts = []\n",
    "prev_month_number = start_date.month\n",
    "daily_dicts_for_month = []\n",
    "for i, daily_words in enumerate(all_word_counts_per_day):\n",
    "    current_date = start_date + datetime.timedelta(days=i)\n",
    "    month_number = current_date.month\n",
    "    if month_number != prev_month_number:\n",
    "        monthly_counts.append(sum_dictionary_values(daily_dicts_for_month))\n",
    "        daily_dicts_for_month = []\n",
    "        prev_month_number = month_number\n",
    "    else:\n",
    "        daily_dicts_for_month.append(daily_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_popular_words_per_month = []\n",
    "for month in monthly_counts:\n",
    "    most_popular_words_per_month.append(get_topn_most_popular(month, topn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "flattened_list = get_flattened_list(most_popular_words_per_month)\n",
    "index = []\n",
    "for i in range(len(flattened_list)):\n",
    "    if i % topn == 0:\n",
    "        month_num = start_date.month + (int(i / topn) % 12)\n",
    "        if month_num > 12:\n",
    "            month_num %= 12\n",
    "        index.append(calendar.month_name[month_num])\n",
    "    else:\n",
    "        index.append('')\n",
    "pd.DataFrame(flattened_list, columns=['word', 'count'],\n",
    "             index=index).to_csv('{}/top{}_words_monthly.csv'.format(twitter_model_path, topn), encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
