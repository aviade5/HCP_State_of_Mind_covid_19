{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"D:\\iliapl\\topic_modeling\\data\\databases\\bad_actors_Coronavirus_Project_POI_Followers_13-06-20.db\"\n",
    "english_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labeled_authors_df = pd.read_csv(r'D:\\iliapl\\topic_modeling\\data\\databases\\person_organization_classification\\labeled_authors_V10.csv')\n",
    "unlabeled_authors_df = pd.read_csv(r'D:\\iliapl\\topic_modeling\\data\\databases\\person_organization_classification\\all_unlabeled_predictions_using_description_and_SVM_classifier_inbalanced_to_label_V10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_authors = set(labeled_authors_df[labeled_authors_df['author_sub_type'] == 'PERSON']['author_screen_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum confidence for 272668 authors: 0.9693990643687439\n"
     ]
    }
   ],
   "source": [
    "# get X% top confidence authors\n",
    "\n",
    "CONFIDENCE_PERCENTILE = 0.25\n",
    "\n",
    "person_sorted_by_confidence = unlabeled_authors_df[unlabeled_authors_df['str_automatic_prediction'] == 'PERSON'].sort_values(by=['confidence_to_organization'], ascending=True)\n",
    "max_organization_confidence = person_sorted_by_confidence.quantile(CONFIDENCE_PERCENTILE)[0]\n",
    "percentile_person_authors = person_sorted_by_confidence[unlabeled_authors_df['confidence_to_organization'] < max_organization_confidence]\n",
    "\n",
    "person_authors |= set(percentile_person_authors['author_screen_name'])\n",
    "\n",
    "print(f'Minimum confidence for {len(person_authors)} authors: {1 - percentile_person_authors.iloc[-1][\"confidence_to_organization\"]}')\n",
    "\n",
    "\n",
    "# get all authors with confidence > X%\n",
    "\n",
    "#MIN_CONFIDENCE = 0.9\n",
    "\n",
    "#person_authors |= set(unlabeled_authors_df[unlabeled_authors_df['confidence_to_organization'] < (1 - MIN_CONFIDENCE)]['author_screen_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272668"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(person_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "import time\n",
    "import random\n",
    "\n",
    "# read num_tweets random tweets from given path to database\n",
    "# if number not given then read ALL tweets\n",
    "def read_tweets(path, num_tweets=0, read_person_authors_only=False, sample_size=0):\n",
    "    T = time.time()\n",
    "    conn = sql.connect(path)\n",
    "    cur = conn.cursor()\n",
    "    if not read_person_authors_only:\n",
    "        query = \"SELECT post_id, content FROM posts WHERE date > date('2019-12-31') {}\".format('' if num_tweets==0 else 'ORDER BY RANDOM() LIMIT {}'.format(num_tweets))\n",
    "        results = cur.execute(query)\n",
    "        results = results.fetchall()\n",
    "        final_results = results\n",
    "    else:\n",
    "        query = \"SELECT post_id, author, content FROM posts WHERE date > date('2019-12-31') {}\".format('' if num_tweets==0 else 'ORDER BY RANDOM() LIMIT {}'.format(num_tweets))\n",
    "        results = cur.execute(query)\n",
    "        results = results.fetchall()\n",
    "        final_results = []\n",
    "        for post_id, author, content in results:\n",
    "            if author in person_authors:\n",
    "                final_results.append((post_id, content))\n",
    "    \n",
    "    print('Finished reading {} tweets in {} seconds'.format(len(final_results), time.time() - T))  \n",
    "    if sample_size > 0:\n",
    "        final_results = random.sample(final_results, sample_size)\n",
    "    \n",
    "    if sample_size > 0: print('Generated a random sample of {} tweets'.format(sample_size))\n",
    "    return {result[0] : result[1] for result in final_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import re\n",
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "def filter_tweets(tweet_id_dict):\n",
    "    filtered_tweets = []\n",
    "    bad_lang_tweets = []\n",
    "    new_dict = {}\n",
    "    \n",
    "    non_english = 0\n",
    "    \n",
    "    for i, (tweet_id, tweet) in enumerate(tweet_id_dict.items()):\n",
    "        \n",
    "        if i % 50000 == 0:\n",
    "            print('Finished filtering {}/{} tweets'.format(i, len(tweet_id_dict)))\n",
    "        \n",
    "        # REMOVE \\r\\n\n",
    "        tweet = tweet.replace('\\r\\n', ' ')\n",
    "\n",
    "        # REMOVE EMOJIS\n",
    "        for ch in tweet:\n",
    "            if ch in UNICODE_EMOJI:\n",
    "                tweet = tweet.replace(str(ch), '')\n",
    "        \n",
    "        # REMOVE @,&\n",
    "        for word in tweet.split():\n",
    "            if word.startswith('@') or word.startswith('&'):\n",
    "                tweet = tweet.replace(word, '')\n",
    "                \n",
    "        # REMOVE &amp\n",
    "        tweet = tweet.replace('&amp', '')\n",
    "\n",
    "        # REMOVE tweets without any letters\n",
    "        if not re.search('[a-zA-Z]', tweet):\n",
    "            continue\n",
    "\n",
    "        tweet = tweet.strip()\n",
    "        \n",
    "        # if tweet is now empty, nevermind\n",
    "        if not tweet:\n",
    "            continue\n",
    "               \n",
    "        #REMOVE NON-ENGLISH\n",
    "        if english_only:\n",
    "            try:\n",
    "                lang = detect(tweet)\n",
    "            except Exception as e:\n",
    "                bad_lang_tweets.append(tweet)\n",
    "                lang = 'ERROR'\n",
    "            if lang=='en':\n",
    "                new_dict[tweet_id] = tweet\n",
    "            else:\n",
    "                non_english += 1\n",
    "        else:     \n",
    "            new_dict[tweet_id] = tweet        \n",
    "        \n",
    "        \n",
    "    print(\"Couldn't detect language in {} tweets (either URL only or non-english)\".format(len(bad_lang_tweets)))\n",
    "    print('Filtered {} non-English tweets'.format(non_english))\n",
    "    \n",
    "    return new_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(string):\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(string)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    " \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import en_core_web_sm\n",
    "    \n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def nltk_preprocess(tweet_id_dict, lemmatizer_name):\n",
    "    keyword_list = []\n",
    "    new_dict = {}\n",
    "    \n",
    "    try:\n",
    "        with open('corona_keywords.txt', 'r', encoding='utf-8') as file_handle:\n",
    "            keyword_list = file_handle.readlines()\n",
    "            keyword_list = [keyword.replace('\\n', '') for keyword in keyword_list]\n",
    "    except Exception as e:\n",
    "        print(\"Couldn't open corona keywords file:\")\n",
    "        print(e)\n",
    "        print('Keywords WERE NOT REMOVED')\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preprocessed_texts = []\n",
    "    for tweet_id, tweet in tweet_id_dict.items():\n",
    "        tokens = word_tokenize(tweet)\n",
    "        new_text = []\n",
    "        for word in tokens:\n",
    "            word = word.lower()\n",
    "            # remove stopwords, keywords, keep words that are alphanumeric, length greater than 2\n",
    "            if word not in stop_words and word not in keyword_list and word.isalnum() and len(word) > 2 and word != 'http' and word != 'https' and 'covid' not in word:\n",
    "                word = word.replace('coronavirus', '')\n",
    "                new_text.append(word)\n",
    "\n",
    "                \n",
    "        # Lemmatize\n",
    "        \n",
    "        # spacy lemmatization\n",
    "        if lemmatizer_name == 'spacy':\n",
    "            lemmatized = nlp(' '.join(new_text))\n",
    "            new_text = [token.lemma_.lower() for token in lemmatized]\n",
    "\n",
    "        # nltk lemmatization\n",
    "        elif lemmatizer_name == 'nltk':\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            new_text = [lemmatizer.lemmatize(w) for w in new_text]\n",
    "        \n",
    "        else:\n",
    "            print('BAD LEMMATIZER!')\n",
    "        \n",
    "        if new_text:\n",
    "            preprocessed_texts.append(new_text)\n",
    "            new_dict[tweet_id] = new_text\n",
    "            \n",
    "    return new_dict\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "folder_name = 'POI_Followers_13-06-20_PERSON_ONLY_V10'\n",
    "corpus_directory = r'D:\\iliapl\\topic_modeling\\data\\output_data\\lda_corpora\\{}'.format(folder_name)\n",
    "num_tweets = 0\n",
    "\n",
    "if not os.path.exists(corpus_directory):\n",
    "    os.makedirs(corpus_directory)\n",
    "\n",
    "tweet_id_dict = {}\n",
    "\n",
    "preprocess_start = time.time()\n",
    "tweet_list = filter_tweets(read_tweets(path, num_tweets, read_person_authors_only=True, sample_size=0))\n",
    "preprocessed_tweets = nltk_preprocess(tweet_list, 'nltk')\n",
    "print('{} tweets after pre-processing'.format(len(preprocessed_tweets)))\n",
    "\n",
    "# create dictionary for gensim\n",
    "model_dict = Dictionary([content for (tweet_id, content) in preprocessed_tweets.items()])\n",
    "\n",
    "# create corpus for gensim (word -> frequency)\n",
    "corpus = []\n",
    "for tweet_id, doc in preprocessed_tweets.items():\n",
    "    bow = model_dict.doc2bow(doc)\n",
    "    corpus.append(bow)\n",
    "    tweet_id_dict[tweet_id] = bow\n",
    "\n",
    "model_dict.save('{}/dict.id2word'.format(corpus_directory))\n",
    "MmCorpus.serialize('{}/corpus.mm'.format(corpus_directory), corpus, id2word=model_dict)\n",
    "with open('{}/post_id_bow_dict.json'.format(corpus_directory), 'w') as json_file:\n",
    "    json.dump(tweet_id_dict, json_file)\n",
    "\n",
    "print('pre-processing took {} seconds'.format(time.time() - preprocess_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
